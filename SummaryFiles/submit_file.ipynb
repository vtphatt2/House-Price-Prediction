{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INTRODUCTION TO DATA SCIENCE**\n",
    "## Final Projects - House Price Prediction - 22KDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ABTRACT** \n",
    "This project aims to develop an accurate house price prediction model using data collected from the website https://batdongsan.vn/ban-nha/. By applying both learned and newly researched data analysis methods and techniques, we evaluated the effectiveness of various predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **INTRODUCTION** \n",
    "In the context of an increasingly volatile real estate market, accurate house price prediction is an urgent need. The goal of this project is to build a house price prediction model based on online data, providing users with clear insights and accurate forecasts about real estate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LITERATURE REVIEW**\n",
    "Previous studies have employed various methods for house price prediction, ranging from simple regression models to complex machine learning techniques such as Random Forest and Gradient Boosting. This study builds upon and further develops these proven methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TECH STACKS**\n",
    "\n",
    "* **Programming Languages:** Python\n",
    "\n",
    "* **Libraries:** BeautifulSoup, Pandas, Numpy, Matplotlib, Seaborn, Scikit-Learn, Statsmodels,...\n",
    "\n",
    "* **IDE:** Visual Studio Code, Google Colab, PyCharm\n",
    "\n",
    "* **Presentation:** Canva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Libraries we used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import KernelDensity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA AND METHODS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing and Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, our group will report on the issues we encountered during the data preprocessing stage and how we handled them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Issues encountered during the data processing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the input file (containing raw data crawled from the web), we will perform data preprocessing steps to clean the data, and then output the results into an output file (containing the cleaned data after all preprocessing steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn tới file CSV đầu vào \n",
    "file_input = '../Data/temp_data.csv'\n",
    "\n",
    "# Đường dẫn tới file CSV đầu ra\n",
    "file_output = '../Data/clean_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 01:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initially crawling data from the web (raw data), I encountered an error that prevented the execution of the `pd.read_csv` command. Upon a preliminary inspection, I discovered that the error was due to extra quotation marks in the data string, which caused the data structure to be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm sửa lỗi dư hoặc thiếu dấu ngoặc kép trong file CSV\n",
    "def fix_csv_quotes(file_input, file_output):\n",
    "    # Mở file đầu vào để đọc và file đầu ra để ghi\n",
    "    with open(file_input, 'r', encoding = 'utf-8') as infile, open(file_output, 'w', newline = '', encoding = 'utf-8') as outfile:\n",
    "        # Đọc tất cả các dòng từ file đầu vào\n",
    "        reader = infile.readlines()\n",
    "        # Tạo writer để ghi vào file đầu ra\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Lặp qua từng dòng trong file đầu vào\n",
    "        for line in reader:\n",
    "            # Kiểm tra số lượng dấu ngoặc kép trong dòng\n",
    "            num_quotes = line.count('\"')\n",
    "            # Nếu số lượng dấu ngoặc kép là số lẻ, thêm một dấu ngoặc kép ở cuối dòng\n",
    "            if num_quotes % 2 != 0:\n",
    "                line = line.strip() + '\"\\n'\n",
    "            # Ghi lại dòng đã sửa vào file đầu ra\n",
    "            writer.writerow(next(csv.reader([line], skipinitialspace = True)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `fix_csv_quotes` function to appropriately adjust the pairs of quotation marks (\" \") in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_csv_quotes(file_input, file_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 02:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the columns of the CSV file is not clean; specifically, there are unnecessary characters and special characters that could significantly impact subsequent processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_index = 8  # Chỉ số của cột cần làm sạch (bắt đầu từ 0), cột 8 là cột Description\n",
    "\n",
    "def clean_string(string):\n",
    "\n",
    "    \"\"\"\n",
    "    Hàm này sẽ làm sạch chuỗi bằng cách loại bỏ các biểu tượng và ký tự không cần thiết.\n",
    "    (nhưng giữ lại các kí tự chữ (hoa và thường), số, dấu chấm, dấu phẩy, dấu | )\n",
    "    \n",
    "    string: Chuỗi cần làm sạch.\n",
    "    cleanString: Chuỗi đã được làm sạch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Hàm isinstance check xem data input của mình có phải là string không \n",
    "    if isinstance(string, str):\n",
    "        cleanString = re.sub(r'[^\\w\\s.,|]', '', string) # Xoá tất cả các kí tự ngoại trừ chữ (thường, hoa), số, dấu chấm, dấu phẩy, dấu |\n",
    "        return cleanString\n",
    "    else:\n",
    "        return string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_csv(file_input, file_output, column_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Hàm này sẽ làm sạch một cột cụ thể trong file CSV, do có những cột khác (VD cột Postdate có dấu : ), \n",
    "    Vì vậy nếu apply full file csv nó sẽ fix hết cả những cột không cần thiết -> chỉ áp dụng chính xác cột cần xử lí thôi.\n",
    "\n",
    "    input_csv (str): Đường dẫn tới file CSV đầu vào (temp_data).\n",
    "    output_csv (str): Đường dẫn tới file CSV đầu ra (clean_data).\n",
    "    column_index (int): Chỉ số của cột cần làm sạch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Đọc file CSV vào DataFrame\n",
    "    df = pd.read_csv(file_input, on_bad_lines = 'skip')\n",
    "\n",
    "    # Kiểm tra xem chỉ số cột có hợp lệ không\n",
    "    if column_index < len(df.columns):\n",
    "        # Áp dụng hàm làm sạch cho cột cụ thể\n",
    "        col = df.columns[column_index]\n",
    "        df[col] = df[col].apply(clean_string)\n",
    "    else:\n",
    "        print(f\"Invalid column index !\")\n",
    "\n",
    "    # Lưu lại DataFrame đã được làm sạch thành file CSV mới\n",
    "    df.to_csv(file_output, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file_csv(file_input, file_output, column_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 03:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from the **`Title`** and **`Description`** fields to fill in the positions where the data is null using `regular expressions` (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc lại file csv, xoá các dòng bị lỗi \n",
    "# (mình nhận thấy số lượng dòng bị lỗi không nhiều ~ 10 dòng nên mình sẽ skip luôn vì tụi mình có nhiều data mà =))))\n",
    "df = pd.read_csv('../Data/clean_data.csv', on_bad_lines = 'skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay thế \"Thoả thuận\" bằng NaN cho dễ xử lí về sau (tạm thời)\n",
    "df['Price'] = df['Price'].replace('TT', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biểu thức chính quy để trích xuất thông tin giá\n",
    "price_pattern = re.compile(\n",
    "    r'Giá[^0-9]*([0-9]+[.,]?[0-9]*)\\s*(Tỷ|Triệu)', \n",
    "    re.IGNORECASE\n",
    ")\n",
    "# 'Giá' - Tìm từ \"Giá\"\n",
    "# '[^0-9]*' - Không chứa ký tự số\n",
    "# '([0-9]+[.,]?[0-9]*)' - Nhóm các số, có thể có dấu phẩy hoặc dấu chấm\n",
    "# '\\s*' - Bỏ qua các khoảng trắng\n",
    "# '(Tỷ|Triệu)' - Tìm từ \"Tỷ\" hoặc \"Triệu\"\n",
    "# 're.IGNORECASE' - Không phân biệt chữ hoa chữ thường\n",
    "\n",
    "\n",
    "# Biểu thức chính quy để trích xuất thông tin diện tích\n",
    "area_pattern = re.compile(\n",
    "    r'(\\d+\\.?\\d*)\\s*x\\s*(\\d+\\.?\\d*)|\\((\\d+\\.?\\d*)m2\\)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "# '(\\d+\\.?\\d*)\\s*x\\s*(\\d+\\.?\\d*)' - Tìm diện tích dưới dạng \"chiều dài x chiều rộng\"\n",
    "# '(\\d+\\.?\\d*)' - Nhóm tìm các số, có thể có dấu chấm thập phân\n",
    "# '\\s*x\\s*' - Tìm ký tự \"x\", có thể có khoảng trắng\n",
    "# '(\\d+\\.?\\d*)' - Nhóm tìm các số phía sau ký tự \"x\"\n",
    "# '|' - Hoặc\n",
    "# '\\((\\d+\\.?\\d*)m2\\)' - Tìm diện tích dưới dạng \"(xxx m2)\"\n",
    "# '\\(' - Dấu mở ngoặc đơn\n",
    "# '(\\d+\\.?\\d*)' - Nhóm tìm các số, có thể có dấu chấm thập phân\n",
    "# 'm2' - Ký tự \"m2\"\n",
    "# '\\)' - Dấu đóng ngoặc đơn\n",
    "# 're.IGNORECASE' - Không phân biệt chữ hoa chữ thường\n",
    "\n",
    "\n",
    "# Biểu thức chính quy để trích xuất thông tin số phòng ngủ\n",
    "bedroom_pattern = re.compile(\n",
    "    r'(\\d+)\\s*PN',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "# '(\\d+)' - Nhóm tìm một hoặc nhiều chữ số\n",
    "# '\\s*' - Bỏ qua các khoảng trắng\n",
    "# 'PN' - Tìm từ \"PN\" (phòng ngủ)\n",
    "# 're.IGNORECASE' - Không phân biệt chữ hoa chữ thường\n",
    "\n",
    "\n",
    "# Biểu thức chính quy để trích xuất thông tin số nhà vệ sinh\n",
    "wc_pattern = re.compile(\n",
    "    r'(\\d+)\\s*WC',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "# '(\\d+)' - Nhóm tìm một hoặc nhiều chữ số\n",
    "# '\\s*' - Bỏ qua các khoảng trắng\n",
    "# 'WC' - Tìm từ \"WC\" (nhà vệ sinh)\n",
    "# 're.IGNORECASE' - Không phân biệt chữ hoa chữ thường\n",
    "\n",
    "\n",
    "# Biểu thức chính quy để trích xuất thông tin số tầng\n",
    "floor_pattern = re.compile(\n",
    "    r'(\\d+)\\s*(lầu|tầng|tang|tâng|lau|lâu|sàn lầu)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "# '(\\d+)' - Nhóm tìm một hoặc nhiều chữ số\n",
    "# '\\s*' - Bỏ qua các khoảng trắng\n",
    "# '(lầu|tầng)' - Tìm từ \"lầu\" hoặc \"tầng\"\n",
    "# 're.IGNORECASE' - Không phân biệt chữ hoa chữ thường\n",
    "\n",
    "\n",
    "# Biểu thức chính quy để trích xuất thông tin pháp lý của ngôi nhà\n",
    "legal_pattern = re.compile(\n",
    "    r'\\b(pháp lý|sổ|so hong|phap ly)\\b',\n",
    "    re.IGNORECASE)\n",
    "#\\b: Ký tự biên (word boundary), đảm bảo rằng từ được tìm kiếm là từ độc lập, không phải là một phần của từ khác.\n",
    "#(pháp lý|sổ): Nhóm các từ cần tìm kiếm, cụ thể là \"sổ\", \"pháp lý\", \"sổ hồng\"\n",
    "# 're.IGNORECASE' - Không phân biệt chữ hoa chữ thường\n",
    "\n",
    "\n",
    "# Biểu thức chính quy để trích xuất thông tin xem nhà ở mặt tiền hay hẻm\n",
    "frontage_pattern = re.compile(r'\\b(mặt tiền|hẻm|mat tien|hem)\\b', re.IGNORECASE)\n",
    "#\\b: Ký tự biên (word boundary), đảm bảo rằng từ được tìm kiếm là từ độc lập, không phải là một phần của từ khác.\n",
    "#(pmặt tiền|hẻm|mat tien|hem): Nhóm các từ cần tìm kiếm, cụ thể là \"mặt tiền\", \"\"hẻm\"\n",
    "# 're.IGNORECASE' - Không phân biệt chữ hoa chữ thường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để kiểm tra xem một câu có chứa các cụm từ \"xây được\", \"xây\", \"được xây\" hay không\n",
    "# Do các thông tin nay gây trích xuất sai: Khu vực xây được ... tầng không phải là số tầng của căn nhà\n",
    "def contains_build_phrase(text):\n",
    "    return bool(re.search(r'\\b(xây được|xây|được xây)\\b', text, re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để loại bỏ các số sau cụm từ \"xây được\", \"xây\", \"được xây\"\n",
    "def remove_numbers(text):\n",
    "    pattern = re.compile(r'\\b(xây được|xây|được xây)\\b.*?(\\d+)', re.IGNORECASE)\n",
    "    return pattern.sub(r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để xử lý các trường hợp đặc biệt như \"1 trệt 1 lầu\", \"1 trệt 1 lửng\"\n",
    "def special_cases(text):\n",
    "    if re.search(r'\\b1 trệt 1 lầu\\b', text, re.IGNORECASE):\n",
    "        return 2\n",
    "    if re.search(r'\\b1 trệt 1 lửng\\b', text, re.IGNORECASE):\n",
    "        return 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để trích xuất thông tin từ một chuỗi văn bản\n",
    "def extract_info(text):\n",
    "    # Kiểm tra nếu text là None hoặc NaN thì gán giá trị rỗng\n",
    "    if text is None or pd.isna(text):\n",
    "        text = ''  # Khởi tạo giá trị rỗng nếu text là None hoặc NaN\n",
    "    # Nếu text không phải là chuỗi, chuyển đổi nó sang chuỗi\n",
    "    elif not isinstance(text, str):\n",
    "        text = str(text)  # Chuyển đổi giá trị sang chuỗi nếu không phải là chuỗi\n",
    "    \n",
    "    # Loại bỏ các số sau cụm từ \"xây được\", \"xây\", \"được xây\"\n",
    "    filtered_text = remove_numbers(text)\n",
    "    \n",
    "    # Kiểm tra các trường hợp đặc biệt\n",
    "    floors = special_cases(filtered_text)\n",
    "    \n",
    "    # Nếu không có trường hợp đặc biệt, tiếp tục trích xuất thông tin thông thường\n",
    "    if floors is None:\n",
    "        # Tìm kiếm các mẫu trong text\n",
    "        price_match = price_pattern.search(filtered_text)\n",
    "        area_match = area_pattern.search(filtered_text)\n",
    "        bedroom_match = bedroom_pattern.search(filtered_text)\n",
    "        wc_match = wc_pattern.search(filtered_text)\n",
    "        legal_match = legal_pattern.search(filtered_text)\n",
    "        frontage_match = frontage_pattern.search(filtered_text)\n",
    "        \n",
    "        # Trích xuất số tầng nếu tìm thấy\n",
    "        floor_matches = floor_pattern.findall(filtered_text)\n",
    "        if floor_matches:\n",
    "            for match in floor_matches:\n",
    "                floor_count = int(match[0])\n",
    "                if match[1].lower() in ['lầu', 'lau', 'lâu', 'sàn lầu']:\n",
    "                    floors = floor_count + 1  # Tăng giá trị lên 1 nếu là \"lầu\"\n",
    "                    break\n",
    "                elif match[1].lower() in ['tầng', 'tang', 'tâng']:\n",
    "                    if floor_count > 15:\n",
    "                        lầu_match = re.search(r'(\\d+)\\s*(lầu|lau|lâu|sàn lầu)', filtered_text, re.IGNORECASE)\n",
    "                        if lầu_match:\n",
    "                            floors = int(lầu_match.group(1)) + 1\n",
    "                            break\n",
    "                    else:\n",
    "                        floors = floor_count\n",
    "        \n",
    "        price = None\n",
    "        # Nếu tìm thấy giá\n",
    "        if price_match:\n",
    "            number = price_match.group(1).replace(',', '.')\n",
    "            unit = price_match.group(2)\n",
    "            # Chuyển đổi giá trị sang VND dựa trên đơn vị\n",
    "            if unit and unit.lower() in ['tỷ', 'tỷ', 'tỉ']:\n",
    "                price = float(number) * 1_000_000_000  # Chuyển đổi tỷ sang VND\n",
    "            elif unit and unit.lower() in ['triệu', 'triệu']:\n",
    "                price = float(number) * 1_000_000  # Chuyển đổi triệu sang VND\n",
    "            else:\n",
    "                price = float(number)  # Nếu không có đơn vị, chỉ trả về số dưới dạng float\n",
    "        \n",
    "        area = None\n",
    "        # Nếu tìm thấy diện tích\n",
    "        if area_match:\n",
    "            if area_match.group(3):\n",
    "                area = float(area_match.group(3))  # Diện tích tổng (150m2)\n",
    "            else:\n",
    "                width = float(area_match.group(1))\n",
    "                length = float(area_match.group(2))\n",
    "                area = width * length  # Tính diện tích từ chiều dài và chiều rộng\n",
    "        \n",
    "        # Trích xuất số phòng ngủ nếu tìm thấy\n",
    "        bedrooms = bedroom_match.group(1) if bedroom_match else None\n",
    "        # Trích xuất số nhà vệ sinh nếu tìm thấy\n",
    "        wcs = wc_match.group(1) if wc_match else None\n",
    "        legal = 1 if legal_match else None\n",
    "        frontage = 1 if frontage_match and frontage_match.group(0).lower() in ['mặt tiền', 'mat tien'] else 0 if frontage_match and frontage_match.group(0).lower() in ['hẻm', 'hem'] else None\n",
    "    else:\n",
    "        # Gán giá trị mặc định cho các thông tin khác nếu không có thông tin từ văn bản\n",
    "        price = None\n",
    "        area = None\n",
    "        bedrooms = None\n",
    "        wcs = None\n",
    "        legal = None\n",
    "        frontage = None\n",
    "    \n",
    "    # Trả về kết quả dưới dạng Series của pandas\n",
    "    return pd.Series([price, area, bedrooms, wcs, floors, legal, frontage])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để trích xuất thông tin từ cả 'Title' và 'Description'\n",
    "def extract_info_from_row(row):\n",
    "    title_info = extract_info(row['Title'])\n",
    "    description_info = extract_info(row['Description'])\n",
    "    \n",
    "    # Nếu thông tin từ Description không có, lấy thông tin từ Title\n",
    "    final_info = title_info.combine_first(description_info)\n",
    "    \n",
    "    return final_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Floors' not in df.columns:\n",
    "    df['Floors'] = None\n",
    "if 'Legal_status' not in df.columns:\n",
    "    df['Legal_status'] = None\n",
    "if 'Frontage' not in df.columns:\n",
    "    df['Frontage'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áp dụng hàm extract_info_from_row lên từng hàng của DataFrame và lưu kết quả vào các cột mới\n",
    "df[['Extracted_Price', 'Extracted_Area', 'Extracted_Bedrooms', 'Extracted_WCs', 'Extracted_Floors', 'Extracted_Legal_status', 'Extracted_Frontage']] = df.apply(extract_info_from_row, axis = 1)\n",
    "\n",
    "# Chuyển đổi cột 'Price' sang kiểu số, nếu có lỗi thì gán giá trị NaN\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors ='coerce')\n",
    "\n",
    "# Cập nhật cột 'Price' theo điều kiện: nếu 'Price' ban đầu là NaN hoặc lớn hơn 1000 thì lấy giá trị từ 'Extracted_Price'\n",
    "df['Price'] = df.apply(lambda row: row['Extracted_Price'] if pd.isnull(row['Price']) or row['Price'] > 1000 else row['Price'], axis = 1)\n",
    "\n",
    "# Hàm để điền thông tin số phòng ngủ (bedrooms), số nhà vệ sinh (WCs) và số tầng (floors) từ các giá trị đã trích xuất\n",
    "def fill_info(row):\n",
    "    if pd.isnull(row['Bedrooms']):\n",
    "        row['Bedrooms'] = row['Extracted_Bedrooms']  # Nếu 'Bedrooms' ban đầu là NaN thì lấy giá trị từ 'Extracted_Bedrooms'\n",
    "    if pd.isnull(row['WCs']):\n",
    "        row['WCs'] = row['Extracted_WCs']  # Nếu 'WCs' ban đầu là NaN thì lấy giá trị từ 'Extracted_WCs'\n",
    "    if pd.isnull(row['Floors']):\n",
    "        row['Floors'] = row['Extracted_Floors']  # Nếu 'Floors' ban đầu là NaN thì lấy giá trị từ 'Extracted_Floors'\n",
    "    if pd.isnull(row['Legal_status']):\n",
    "        row['Legal_status'] = row['Extracted_Legal_status']  # Nếu 'Legal_status' ban đầu là NaN thì lấy giá trị từ 'Extracted_Legal_status'\n",
    "    if pd.isnull(row['Frontage']):\n",
    "        row['Frontage'] = row['Extracted_Frontage']  # Nếu 'Frontage' ban đầu là NaN thì lấy giá trị từ 'Extracted_Frontage'\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áp dụng hàm fill_info lên từng hàng của DataFrame\n",
    "df = df.apply(fill_info, axis = 1)\n",
    "\n",
    "# Xuất DataFrame mới ra file CSV\n",
    "df.to_csv(file_output, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loại bỏ cột No và cột Title không cần thiết \n",
    "df = df.drop(columns = df.columns[[0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 04:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform detailed processing, calculations, and filling of missing values. Additionally, add a feature called **`Price_per_sqm`** (price per square meter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi các cột 'Price' và 'Area' sang kiểu số (float) để thực hiện các bước xử lí, tính toán dễ dàng hơn.\n",
    "df['Price'] = df['Price'].astype(float)\n",
    "df['Area'] = df['Area'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xử lý cột Price\n",
    "def process_price(df):\n",
    "    def convert_price(price):\n",
    "        if pd.isnull(price):\n",
    "            return price\n",
    "        price = float(price)\n",
    "        if price > 1e9:\n",
    "            return price / 1e9  # Nếu giá trị lớn hơn 1 tỷ, chuyển sang đơn vị tỷ\n",
    "        elif price > 1e6:\n",
    "            return price / 1e6  # Nếu giá trị lớn hơn 1 triệu, chuyển sang đơn vị triệu\n",
    "        elif price > 1e3:\n",
    "            return price / 1e3  # Nếu giá trị lớn hơn 1 nghìn, chuyển sang đơn vị nghìn\n",
    "        return price  # Trả về giá trị gốc nếu nhỏ hơn 1 nghìn\n",
    "    \n",
    "    df['Price'] = df['Price'].apply(convert_price)  # Áp dụng hàm convert_price cho cột 'Price'\n",
    "    return df\n",
    "\n",
    "df = process_price(df)\n",
    "\n",
    "# Hàm tính giá trên một mét vuông\n",
    "def price_per_sqm(df):\n",
    "    df['Price_per_sqm'] = df.apply(lambda row: row['Price'] / row['Area'] if pd.notnull(row['Price']) and pd.notnull(row['Area']) else np.nan, axis = 1)\n",
    "    # Tạo cột 'Price_per_sqm' bằng cách lấy 'Price' chia cho 'Area' nếu cả hai đều không bị null, ngược lại trả về NaN\n",
    "    return df\n",
    "\n",
    "\n",
    "# Hàm điền giá trị thiếu\n",
    "def fill_miss_vals(df):\n",
    "    # Tính giá trị trung bình của 'Price', 'Area', và 'Price_per_sqm' theo 'District'\n",
    "    district_means = df.groupby('District').mean(numeric_only = True)\n",
    "    \n",
    "    # Điền giá trị thiếu cho 'Price' bằng giá trị trung bình của 'District'\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isnull(row['Price']):\n",
    "            df.at[idx, 'Price'] = district_means.loc[row['District'], 'Price']\n",
    "    \n",
    "    # Tính lại giá trên một mét vuông\n",
    "    df = price_per_sqm(df)\n",
    "    \n",
    "    # Điền giá trị thiếu cho 'Area' và 'Price' dựa trên 'Price_per_sqm'\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isnull(row['Area']) and pd.notnull(row['Price']):\n",
    "            df.at[idx, 'Area'] = row['Price'] / row['Price_per_sqm']  # Tính diện tích dựa trên giá trị 'Price' và 'Price_per_sqm'\n",
    "        elif pd.isnull(row['Price']) and pd.notnull(row['Area']):\n",
    "            df.at[idx, 'Price'] = row['Area'] * row['Price_per_sqm']  # Tính giá trị 'Price' dựa trên 'Area' và 'Price_per_sqm'\n",
    "        elif pd.isnull(row['Price']) and pd.isnull(row['Area']):\n",
    "            df.at[idx, 'Price'] = district_means.loc[row['District'], 'Price']  # Điền 'Price' bằng giá trị trung bình của quận\n",
    "            df.at[idx, 'Area'] = district_means.loc[row['District'], 'Area']  # Điền 'Area' bằng giá trị trung bình của quận\n",
    "    \n",
    "    \n",
    "    # Xử lý các giá trị 'Area' vẫn còn thiếu\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isnull(row['Area']):  \n",
    "            df.at[idx, 'Area'] = district_means.loc[row['District'], 'Area']\n",
    "\n",
    "    # Điền giá trị thiếu cho 'Price_per_sqm' nếu còn thiếu\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isnull(row['Price_per_sqm']) and pd.notnull(row['Price']) and pd.notnull(row['Area']):\n",
    "            df.at[idx, 'Price_per_sqm'] = row['Price'] / row['Area']  # Tính lại 'Price_per_sqm' nếu thiếu và các giá trị khác không bị null\n",
    "    \n",
    "\n",
    "    # Làm tròn các giá trị vừa điền vào\n",
    "    df['Price'] = df['Price'].round(2)  # Làm tròn giá trị 'Price' đến 2 chữ số thập phân\n",
    "    df['Area'] = df['Area'].round(2)  # Làm tròn giá trị 'Area' đến 2 chữ số thập phân\n",
    "    df['Price_per_sqm'] = df['Price_per_sqm'].round(2)  # Làm tròn giá trị 'Price_per_sqm' đến 2 chữ số thập phân\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra giá trị null trong cột 'Area' và 'Price'\n",
    "null_area = df['Area'].isnull().sum()\n",
    "print(null_area)\n",
    "null_price = df['Price'].isnull().sum()\n",
    "print(null_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm cột giá trên 1 mét vuông\n",
    "df = price_per_sqm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of discarding unreasonable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để cập nhật cột Price theo điều kiện price_per_sqm > 1\n",
    "def update_price(row):\n",
    "    # Kiểm tra nếu 'Area' không phải là None và lớn hơn 0\n",
    "    if row['Area'] and row['Area'] > 0:  # Đảm bảo diện tích không phải là None hoặc 0\n",
    "        # Tính giá trên mỗi mét vuông\n",
    "        price_per_sqm = row['Price'] / row['Area']\n",
    "        # Nếu giá trên mỗi mét vuông lớn hơn 1\n",
    "        if price_per_sqm > 1:\n",
    "            # Trả về giá trị từ 'Extracted_Price'\n",
    "            return row['Extracted_Price']\n",
    "    # Nếu không thỏa mãn điều kiện, trả về giá trị ban đầu của 'Price'\n",
    "    return row['Price']\n",
    "\n",
    "# Áp dụng hàm update_price lên từng hàng của DataFrame\n",
    "df['Price'] = df.apply(update_price, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Điền giá trị thiếu cho 'Price' và 'Area'\n",
    "df = fill_miss_vals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện xử lí các giá trị ở cột Price để dữ liệu chuẩn hơn, hợp lý hơn.\n",
    "df = process_price(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 05:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next issue with the data is that there are too many duplicate rows.\n",
    "\n",
    "Initially, after using Python's built-in command to remove duplicates `drop_duplicates()` (removing rows that are 100% identical), I realized that many duplicates still remained due to the following reasons:\n",
    "* The same house was posted multiple times with different descriptions.\n",
    "* The same house was posted at different times or on different dates.\n",
    "* ...\n",
    "\n",
    "These reasons caused the duplicates not to be completely removed, as Python's built-in function only deletes rows that are 100% identical. Since the **`Postdate`** and **`Description`** fields had slight differences, the built-in function `drop_duplicates()` could not effectively handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tasks we handled focused on the **`Postdate`** and **`Description`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lí cột Postdate (chỉ giữ lại Ngày xoá Giờ)\n",
    "# Chuyển đổi cột 'Postdate' sang kiểu datetime\n",
    "df['Postdate'] = pd.to_datetime(df['Postdate'])\n",
    "\n",
    "# Tạo một cột mới chỉ chứa ngày\n",
    "df['Date'] = df['Postdate'].dt.date\n",
    "\n",
    "# Tìm vị trí (index) của cột 'Postdate'\n",
    "col_index = df.columns.get_loc('Postdate')\n",
    "\n",
    "# Xóa cột 'Postdate' cũ \n",
    "df = df.drop(columns = ['Postdate'])\n",
    "\n",
    "# Chèn cột 'Date' mới vào đúng vị trí cũ của cột 'Postdate'\n",
    "df.insert(col_index, 'Postdate', df.pop('Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lại những thay đổi vào file clean_data.csv\n",
    "df.to_csv(file_output, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, after running the program, an additional error appeared (Unusual line terminators). After some research, we found a solution using the `remove_unusual_line_terminators` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unusual_line_terminators(file_input, file_output):\n",
    "    with open(file_input, 'r', encoding='utf-8') as infile:\n",
    "        content = infile.read()\n",
    "\n",
    "    # Loại bỏ các ký tự kết thúc dòng không bình thường (LS và PS)\n",
    "    cleaned_content = content.replace('\\u2028', '\\n').replace('\\u2029', '\\n')\n",
    "\n",
    "    with open(file_output, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        outfile.write(cleaned_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    # Đảm bảo rằng các giá trị trong 'Postdate' và 'Description' là chuỗi\n",
    "    df['Postdate'] = df['Postdate'].astype(str)\n",
    "    df['Description'] = df['Description'].astype(str)\n",
    "    \n",
    "    # Bước 1: Loại bỏ các dòng trùng lặp mà tất cả các cột đều giống nhau\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Bước 2: Nhóm các dòng theo các cột 'Price', 'Area', 'Bedrooms', 'WCs', 'District'\n",
    "    grouped = df.groupby(['Price', 'Area', 'Bedrooms', 'WCs', 'District'], dropna = False)\n",
    "    \n",
    "    # Danh sách các chỉ số của các dòng cần giữ lại\n",
    "    indices_to_keep = set(df.index)\n",
    "\n",
    "    def similar(a, b):\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "    \n",
    "    # Lặp qua từng nhóm để kiểm tra sự tương đồng\n",
    "    for _, group in grouped:\n",
    "        group_indices = list(group.index)\n",
    "        for i in range(len(group_indices)):\n",
    "            for j in range(i + 1, len(group_indices)):\n",
    "                idx1, idx2 = group_indices[i], group_indices[j]\n",
    "                row1, row2 = df.loc[idx1], df.loc[idx2]\n",
    "                # Kiểm tra sự tương đồng của 'Description' và ngày 'Postdate'\n",
    "                postdate_similarity = similar(row1['Postdate'], row2['Postdate'])\n",
    "                description_similarity = similar(row1['Description'], row2['Description'])\n",
    "                if postdate_similarity > 0.75 and description_similarity > 0.5:\n",
    "                    indices_to_keep.discard(idx2)\n",
    "    \n",
    "    # Chuyển đổi tập hợp các chỉ số cần giữ lại thành danh sách\n",
    "    indices_to_keep = list(indices_to_keep)\n",
    "    \n",
    "    # Lọc lại DataFrame với các chỉ số cần giữ lại\n",
    "    df_cleaned = df.loc[indices_to_keep].reset_index(drop = True)\n",
    "    \n",
    "    # Trả về DataFrame đã loại bỏ các dòng trùng lặp\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áp dụng hàm remove_duplicates lên DataFrame đã được làm sạch\n",
    "df = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 06:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old posts can cause inaccuracies with the current timeframe, so we decided to delete outdated data (specifically, house sale posts from 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem kiểu dữ liệu của cột \"Postdate\"\n",
    "df['Postdate'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển kiểu dữ liệu của cột \"Postdate\" sang dạng datetime và check lại kiểu dữ liệu 1 lần nữa\n",
    "df['Postdate'] = pd.to_datetime(df['Postdate'])\n",
    "df['Postdate'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loại bỏ các data có ngày đăng tin là 2022 và lưu lại vào file csv\n",
    "df = df[df['Postdate'].dt.year != 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 07:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above data processing steps, upon re-evaluating the data, we noticed that some data entries were still unreasonable. Therefore, we proceeded to delete these unreasonable data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid(df):\n",
    "    return df[(df['Price_per_sqm'] <= 2) & (df['Price_per_sqm'] >= 0.03)]\n",
    "\n",
    "df = remove_invalid(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that house prices were not reasonable in some areas, so we adjusted the prices to be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các quận cần điều chỉnh\n",
    "districts = ['binh-chanh', 'binh-tan', 'hoc-mon', 'cu-chi', 'tan-binh', 'quan-8', 'quan-12']\n",
    "\n",
    "def adjust_price(row):\n",
    "    if any(district in row['District'] for district in districts) and row['Price_per_sqm'] > 1:\n",
    "        return row['Price'] / 1_000\n",
    "    return row['Price']\n",
    "\n",
    "# Áp dụng hàm adjust_price_for_district\n",
    "df['Price'] = df.apply(adjust_price, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['Extracted_Price', 'Extracted_Area', 'Extracted_Bedrooms', 'Extracted_WCs', 'Extracted_Floors', 'Extracted_Legal_status','Extracted_Frontage']\n",
    "# Xóa các cột\n",
    "df = df.drop(columns = cols_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu lại DataFrame vào file CSV\n",
    "df.to_csv(file_output, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 08:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the **`District`** column into numerical values and extract the month from the **`Postdate`** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode cột District\n",
    "# Đầu tiên, xác định các giá trị duy nhất trong cột District\n",
    "district_unique_values = df['District'].unique()\n",
    "\n",
    "# Tạo một ánh xạ từ tên quận thành các giá trị số\n",
    "district_mapping = {district: i for i, district in enumerate(district_unique_values)}\n",
    "\n",
    "# Áp dụng ánh xạ vào cột District\n",
    "df['District_encoded'] = df['District'].map(district_mapping)\n",
    "\n",
    "# Encode cột Postdate thành tháng\n",
    "df['Month'] = df['Postdate'].dt.month\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị thống kê mô tả cho các cột số\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị số lượng giá trị thiếu cho mỗi cột\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Issue 09:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained **K-Nearest Neighbors (KNN)** models to predict missing values for **`Floors`**, **`Bedrooms`** and **`WCs`** in dataset using features: **`Area`**, **`Price_per_sqm`**, **`District_encoded`**, and **`Month`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn các đặc trưng\n",
    "features = ['Price', 'Area', 'Price_per_sqm', 'District_encoded', 'Month']\n",
    "\n",
    "# Chuyển đổi 'Floors' thành số, lỗi chuyển đổi thành NaN\n",
    "df['Floors'] = pd.to_numeric(df['Floors'], errors = 'coerce')\n",
    "\n",
    "# Loại bỏ các dòng có giá trị thiếu trong các đặc trưng và target\n",
    "data_floors = df.dropna(subset = ['Floors'] + features)\n",
    "\n",
    "# Chia dữ liệu thành các tập huấn luyện và kiểm tra\n",
    "X_floors = data_floors[features]\n",
    "y_floors = data_floors['Floors']\n",
    "X_floors_train, X_floors_test, y_floors_train, y_floors_test = train_test_split(X_floors, y_floors, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Huấn luyện mô hình KNN cho Floors\n",
    "knn_floors = KNeighborsRegressor(n_neighbors = 5)\n",
    "knn_floors.fit(X_floors_train, y_floors_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán giá trị Floors\n",
    "floors_predicted = knn_floors.predict(X_floors_test)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "mse_floors = mean_squared_error(y_floors_test, floors_predicted)\n",
    "print(f'Mean Squared Error for Floors: {mse_floors}')\n",
    "\n",
    "# Dự đoán giá trị thiếu cho Floors trong toàn bộ dữ liệu\n",
    "floors_missing_indices = df[df['Floors'].isnull()].index\n",
    "X_floors_missing = df.loc[floors_missing_indices, features]\n",
    "floors_missing_predicted = knn_floors.predict(X_floors_missing)\n",
    "\n",
    "# Làm tròn giá trị đã dự đoán về số nguyên gần nhất\n",
    "floors_missing_predicted_rounded = [round(value) for value in floors_missing_predicted]\n",
    "\n",
    "# Điền các giá trị đã dự đoán và làm tròn vào cột tương ứng\n",
    "df.loc[floors_missing_indices, 'Floors'] = floors_missing_predicted_rounded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bedrooms'] = pd.to_numeric(df['Bedrooms'], errors = 'coerce')\n",
    "df['WCs'] = pd.to_numeric(df['WCs'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn các đặc trưng bao gồm cả 'Floors'\n",
    "features_with_floors = ['Price', 'Area', 'Price_per_sqm', 'District_encoded', 'Month', 'Floors']\n",
    "\n",
    "# Loại bỏ các dòng có giá trị thiếu trong các đặc trưng và target\n",
    "data_bedrooms_wcs = df.dropna(subset = ['Bedrooms', 'WCs'] + features_with_floors)\n",
    "\n",
    "# Chia dữ liệu thành các tập huấn luyện và kiểm tra cho Bedrooms\n",
    "X_bedrooms = data_bedrooms_wcs[features_with_floors]\n",
    "y_bedrooms = data_bedrooms_wcs['Bedrooms']\n",
    "X_bedrooms_train, X_bedrooms_test, y_bedrooms_train, y_bedrooms_test = train_test_split(X_bedrooms, y_bedrooms, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Huấn luyện mô hình KNN cho Bedrooms\n",
    "knn_bedrooms = KNeighborsRegressor(n_neighbors = 5)\n",
    "knn_bedrooms.fit(X_bedrooms_train, y_bedrooms_train)\n",
    "\n",
    "# Dự đoán giá trị Bedrooms\n",
    "bedrooms_predicted = knn_bedrooms.predict(X_bedrooms_test)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "mse_bedrooms = mean_squared_error(y_bedrooms_test, bedrooms_predicted)\n",
    "print(f'Mean Squared Error for Bedrooms: {mse_bedrooms}')\n",
    "\n",
    "# Chia dữ liệu thành các tập huấn luyện và kiểm tra cho WCs\n",
    "y_wcs = data_bedrooms_wcs['WCs']\n",
    "X_wcs_train, X_wcs_test, y_wcs_train, y_wcs_test = train_test_split(X_bedrooms, y_wcs, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Huấn luyện mô hình KNN cho WCs\n",
    "knn_wcs = KNeighborsRegressor(n_neighbors = 5)\n",
    "knn_wcs.fit(X_wcs_train, y_wcs_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán giá trị WCs\n",
    "wcs_predicted = knn_wcs.predict(X_wcs_test)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "mse_wcs = mean_squared_error(y_wcs_test, wcs_predicted)\n",
    "print(f'Mean Squared Error for WCs: {mse_wcs}')\n",
    "\n",
    "# Dự đoán và điền giá trị thiếu cho Bedrooms và WCs trong toàn bộ dữ liệu\n",
    "bedrooms_missing_indices = df[df['Bedrooms'].isnull()].index\n",
    "X_bedrooms_missing = df.loc[bedrooms_missing_indices, features_with_floors]\n",
    "bedrooms_missing_predicted = knn_bedrooms.predict(X_bedrooms_missing)\n",
    "\n",
    "wcs_missing_indices = df[df['WCs'].isnull()].index\n",
    "X_wcs_missing = df.loc[wcs_missing_indices, features_with_floors]\n",
    "wcs_missing_predicted = knn_wcs.predict(X_wcs_missing)\n",
    "\n",
    "# Làm tròn giá trị đã dự đoán về số nguyên gần nhất\n",
    "bedrooms_missing_predicted_rounded = [round(value) for value in bedrooms_missing_predicted]\n",
    "wcs_missing_predicted_rounded = [round(value) for value in wcs_missing_predicted]\n",
    "\n",
    "# Điền các giá trị đã dự đoán và làm tròn vào cột tương ứng\n",
    "df.loc[bedrooms_missing_indices, 'Bedrooms'] = bedrooms_missing_predicted_rounded\n",
    "df.loc[wcs_missing_indices, 'WCs'] = wcs_missing_predicted_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đặt các giá trị None cho cột Legal thành 0\n",
    "df['Legal_status'] = df['Legal_status'].fillna(0)\n",
    "\n",
    "df['Frontage_1'] = (df['Frontage'] == 1).astype(int)\n",
    "df['Frontage_0'] = (df['Frontage'] == 0).astype(int)\n",
    "df['Frontage_missing'] = (df['Frontage'] == -1).astype(int)\n",
    "\n",
    "df = df.drop(['Frontage'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị số lượng giá trị thiếu cho mỗi cột\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exploratory Data Analysis (EDA) and Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa các biến số\n",
    "numeric_features = ['Price', 'Area', 'Floors', 'Bedrooms', 'WCs', 'Price_per_sqm', 'District_encoded', 'Month']\n",
    "\n",
    "# Vẽ Histogram cho các biến số\n",
    "fig, axes = plt.subplots(len(numeric_features)//3 + 1, 3, figsize = (15, 10)) \n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(numeric_features):\n",
    "    ax = axes[idx]\n",
    "    ax.hist(df[feature], bins = 30, color = 'LightCoral', edgecolor = 'black')  \n",
    "    ax.set_title(feature)\n",
    "    ax.grid(False)  # Xoá lưới\n",
    "\n",
    "# Hide any unused subplots\n",
    "for ax in axes[len(numeric_features):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "\n",
    "*   Right-skewed distributions: For **`Price`**, **`Area`**, **`Price_per_sqm`**, **`Bedrooms`**, and **`WCs`**, the right-skewed distributions indicate that most properties fall within a lower range for these attributes, with fewer properties having significantly higher values.\n",
    "\n",
    "*   Outliers: There are noticeable outliers, especially in the **`Price`**, **`Area`**, **`Bedrooms`**, and **`WCs`** features, which could affect modeling and analysis.\n",
    "\n",
    "*   Uniform distributions: The more uniform distribution of **`District_encoded`** and **`Month`** suggests a balanced representation of districts and months in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính toán ma trận tương quan mà không bao gồm cột 'Price_per_sqm'\n",
    "corr_matrix = df[numeric_features].corr()\n",
    "\n",
    "# Hiển thị ma trận tương quan\n",
    "print(corr_matrix)\n",
    "\n",
    "# Vẽ Heatmap cho ma trận tương quan\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.heatmap(corr_matrix, annot = True, cmap = 'pink_r', fmt = '.2f', cbar = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   The area of the property is the most significant factor affecting the price, with a strong correlation of `0.68`.\n",
    "\n",
    "*   The number of bedrooms and WCs also significantly impact the price, with correlations of `0.41` and `0.38`, respectively. Additionally, the number of floors shows a moderate impact with a correlation of `0.35`.\n",
    "\n",
    "*   Frontage has a minimal effect on the price, with a weak correlation of `0.08`. The district has a slight negative correlation of `-0.13`, and the month shows almost no impact on the price, with a correlation of `0.02`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một figure với sáu subplots trong hai hàng và ba cột\n",
    "fig, axes = plt.subplots(2, 3, figsize = (18, 12))\n",
    "\n",
    "# Scatter plot giữa Area và Price\n",
    "sns.scatterplot(x = 'Area', y = 'Price', data = df, ax = axes[0, 0], color = 'lightcoral')\n",
    "axes[0, 0].set_title('Scatter Plot of Area vs Price')\n",
    "\n",
    "# Scatter plot giữa Bedrooms và Price\n",
    "sns.scatterplot(x = 'Bedrooms', y = 'Price', data = df, ax = axes[0, 1], color = 'lightcoral')\n",
    "axes[0, 1].set_title('Scatter Plot of Bedrooms vs Price')\n",
    "\n",
    "# Scatter plot giữa WCs và Price\n",
    "sns.scatterplot(x = 'WCs', y = 'Price', data = df, ax = axes[0, 2], color = 'lightcoral')\n",
    "axes[0, 2].set_title('Scatter Plot of WCs vs Price')\n",
    "\n",
    "# Scatter plot giữa Floors và Price\n",
    "sns.scatterplot(x = 'Floors', y = 'Price', data = df, ax = axes[1, 0], color = 'lightcoral')\n",
    "axes[1, 0].set_title('Scatter Plot of Floors vs Price')\n",
    "\n",
    "# Scatter plot giữa District_encoded và Price\n",
    "sns.scatterplot(x = 'District_encoded', y = 'Price', data = df, ax = axes[1, 2], color = 'lightcoral')\n",
    "axes[1, 2].set_title('Scatter Plot of District vs Price')\n",
    "\n",
    "# Điều chỉnh layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia Area thành các nhóm (bins) để dễ so sánh\n",
    "df['Area_group'] = pd.cut(df['Area'], bins = 10, labels = False)\n",
    "\n",
    "# Group Bedrooms and WCs into fewer categories\n",
    "df['Bedrooms_group'] = pd.cut(df['Bedrooms'], bins = [0, 2, 4, 6, 8, 10, float('inf')], labels = ['1-2', '3-4', '5-6', '7-8', '9-10', '11+'])\n",
    "df['WCs_group'] = pd.cut(df['WCs'], bins = [0, 2, 4, 6, 8, 10, float('inf')], labels = ['1-2', '3-4', '5-6', '7-8', '9-10', '11+'])\n",
    "df['Floors_group'] = pd.cut(df['Floors'], bins = [0, 1, 2, 3, 4, 5, float('inf')], labels = ['0-1', '2', '3', '4', '5', '6+'])\n",
    "\n",
    "# Tạo figure với nhiều subplot\n",
    "fig, axes = plt.subplots(nrows = 3, ncols = 2, figsize = (20, 24))\n",
    "\n",
    "# Box plot của Price theo District_encoded\n",
    "sns.boxplot(ax = axes[0, 0], x = 'District_encoded', y = 'Price_per_sqm', data = df)\n",
    "axes[0, 0].set_title('Box Plot of Price per sqm by District')\n",
    "axes[0, 0].set_xlabel('District')\n",
    "axes[0, 0].set_ylabel('Price per sqm')\n",
    "axes[0, 0].tick_params(axis = 'x', rotation = 90)\n",
    "\n",
    "# Box plot của Price theo Month\n",
    "sns.boxplot(ax = axes[0, 1], x = 'Month', y = 'Price_per_sqm', data = df)\n",
    "axes[0, 1].set_title('Box Plot of Price per sqm by Month')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Price per sqm')\n",
    "axes[0, 1].tick_params(axis = 'x', rotation = 90)\n",
    "\n",
    "# Box plot của Price theo Area_group\n",
    "sns.boxplot(ax = axes[1, 0], x = 'Area_group', y = 'Price_per_sqm', data = df)\n",
    "axes[1, 0].set_title('Box Plot of Price per sqm by Area Group')\n",
    "axes[1, 0].set_xlabel('Area Group')\n",
    "axes[1, 0].set_ylabel('Price per sqm')\n",
    "\n",
    "# Box plot của Price theo Bedrooms group\n",
    "sns.boxplot(ax = axes[1, 1], x = 'Bedrooms_group', y = 'Price_per_sqm', data = df)\n",
    "axes[1, 1].set_title('Box Plot of Price per sqm by Bedrooms Group')\n",
    "axes[1, 1].set_xlabel('Bedrooms Group')\n",
    "axes[1, 1].set_ylabel('Price per sqm')\n",
    "\n",
    "# Box plot của Price theo WCs group\n",
    "sns.boxplot(ax = axes[2, 0], x = 'WCs_group', y = 'Price_per_sqm', data = df)\n",
    "axes[2, 0].set_title('Box Plot of Price per sqm by WCs Group')\n",
    "axes[2, 0].set_xlabel('WCs Group')\n",
    "axes[2, 0].set_ylabel('Price per sqm')\n",
    "\n",
    "# Box plot của Price theo Floors group\n",
    "sns.boxplot(ax = axes[2, 1], x = 'Floors_group', y = 'Price_per_sqm', data = df)\n",
    "axes[2, 1].set_title('Box Plot of Price per sqm by Floors Group')\n",
    "axes[2, 1].set_xlabel('Floors Group')\n",
    "axes[2, 1].set_ylabel('Price per sqm')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **Price by District:**\n",
    "\n",
    "     * Significant variation in house prices across districts.\n",
    "\n",
    "     * Presence of high outliers in some districts.\n",
    "\n",
    "*   **Price by Month:**\n",
    "\n",
    "     * Fairly even distribution of prices across months.\n",
    "\n",
    "     * Some months have high outliers.\n",
    "\n",
    "*   **Price by Area Group:**\n",
    "\n",
    "     * Prices tend to increase with larger area groups.\n",
    "\n",
    "     * Higher area groups show more high outliers.\n",
    "\n",
    "*  **Price by Bedrooms:**\n",
    "\n",
    "     * More bedrooms generally correlate with higher prices, but with many outliers.\n",
    "\n",
    "     * Higher bedroom counts (e.g., 9-10, 11+) show higher median prices.\n",
    "\n",
    "*   **Price by WCs:**\n",
    "\n",
    "     * More WCs tend to correlate with higher prices, but with many outliers.\n",
    "\n",
    "     * Higher WC counts (e.g., 9-10, 11+) show higher median prices.\n",
    "\n",
    "*   **Price by Floors:**\n",
    "\n",
    "     * More floors tend to correlate with higher prices, but with many outliers.\n",
    "\n",
    "     * Higher floors counts (e.g., 6+ ) show higher median prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Loại bỏ ngoại lệ cho cột Price\n",
    "df = remove_outliers(df, 'Price')\n",
    "\n",
    "# Xóa các cột không cần thiết\n",
    "columns_to_drop = ['District', 'Postdate','Description','Area_group', 'Bedrooms_group', 'WCs_group', 'Floors_group']\n",
    "df = df.drop(columns = columns_to_drop)\n",
    "\n",
    "# Xóa cột No hiện tại nếu tồn tại và thêm cột No mới\n",
    "df = df.drop(columns = ['No'], errors = 'ignore')\n",
    "df.insert(0, 'No', range(1, len(df) + 1))\n",
    "\n",
    "# Kiểm tra kết quả\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xuất dữ liệu đã xử lý ra file CSV mới\n",
    "df.to_csv('../Data/clean_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kernel Density estimation (KDE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc tệp shapefile sử dụng GeoPandas\n",
    "gdf = gpd.read_file('../Data/gadm41_VNM_2.shp')\n",
    "\n",
    "# Filter for TP.HCM\n",
    "gdf_hcm = gdf[gdf['NAME_1'] == 'Há»\\x93 ChÃ\\xad Minh']\n",
    "\n",
    "# Rename column \"VARNAME2\" to\"District\" \n",
    "gdf_hcm = gdf_hcm.rename(columns = {'VARNAME_2': 'District'})\n",
    "\n",
    "# Reformatting name of district\n",
    "replace_dict = {'Binh Chanh':'binh-chanh', 'Binh Tan' : 'binh-tan','Binh Thanh':'binh-thanh', 'Can Gio':'can-gio', \n",
    " 'Cu Chi':'cu-chi', 'Go Vap':'go-vap', 'Hoc Mon':'hoc-mon', 'Nha Be':'nha-be', 'Phu Nhuan':'phu-nhuan', \n",
    " 'District 1':'quan-1', 'District 10':'quan-10', 'District 11':'quan-11', 'District 12':'quan-12', 'District 2':'quan-2', \n",
    " 'District 3':'quan-3', 'District 4':'quan-4', 'District 5':'quan-5', 'District 6':'quan-6', 'District 7':'quan-7', \n",
    " 'District 8':'quan-8', 'District 9':'quan-9', 'Tan Binh':'tan-binh',  'Tan Phu':'tan-phu', 'Thu Duc':'thu-duc'}\n",
    "gdf_hcm['District'] = gdf_hcm['District'].replace(replace_dict)\n",
    "\n",
    "# Read cleaned file with district\n",
    "df = pd.read_csv('../Data/data_with_district.csv')\n",
    "\n",
    "# Keep columns 'District','Price_per_sqm','Month' in dataframe\n",
    "df = df[['District','Price_per_sqm','Month']]\n",
    "\n",
    "# Merge geodataframe and dataframe to a new file csv contain data and geodata\n",
    "df = gdf_hcm.merge(df, on = 'District')\n",
    "df.to_csv('../Data/data_with_gadm.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/data_with_gadm.csv')\n",
    "\n",
    "# Chuyển đổi cột 'geometry' sang GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(data, geometry = gpd.GeoSeries.from_wkt(data['geometry']))\n",
    "\n",
    "# Trích xuất tọa độ trung tâm của mỗi quận\n",
    "gdf['centroid'] = gdf.geometry.centroid.apply(lambda point: (point.x, point.y))\n",
    "\n",
    "# Trích xuất tọa độ\n",
    "coordinates = np.array(gdf['centroid'].tolist())\n",
    "\n",
    "# Tạo KDE\n",
    "kde = KernelDensity(bandwidth = 0.015, metric = 'euclidean')\n",
    "kde.fit(coordinates)\n",
    "\n",
    "# Đánh giá KDE trên một lưới điểm\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "xgrid = np.linspace(xmin + 0.05, xmax + 0.05, 100)\n",
    "ygrid = np.linspace(ymin + 0.05, ymax + 0.05, 100)\n",
    "X, Y = np.meshgrid(xgrid, ygrid)\n",
    "xy_sample = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "Z = kde.score_samples(xy_sample)\n",
    "Z = np.exp(Z).reshape(X.shape)\n",
    "\n",
    "# Vẽ heatmap\n",
    "plt.figure(figsize = (50, 5))\n",
    "plt.contourf(X, Y, Z, levels = np.linspace(20, 120, 12), cmap = 'Blues')\n",
    "plt.colorbar(label = 'Density')\n",
    "plt.title('Heatmap of House Density by District')\n",
    "\n",
    "# Vẽ bản đồ quận\n",
    "gdf.boundary.plot(ax = plt.gca(), linewidth = 0.5, edgecolor = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nhóm dữ liệu theo cột 'District' và tính giá trị trung bình của 'Price_per_sqm' cho mỗi quận\n",
    "price_per_sqm_mean_by_district = df.groupby('District')['Price_per_sqm'].mean().reset_index()\n",
    "\n",
    "# Đổi tên cột để rõ ràng hơn\n",
    "price_per_sqm_mean_by_district.rename(columns = {'Price_per_sqm': 'price_per_sqm_mean'}, inplace = True)\n",
    "\n",
    "# Hợp nhất dữ liệu số lượng nhà bán với dữ liệu bản đồ hành chính\n",
    "gdf = gdf_hcm.merge(price_per_sqm_mean_by_district, on = 'District')\n",
    "\n",
    "# Vẽ bản đồ nhiệt\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 5))\n",
    "\n",
    "gdf.plot(column = 'price_per_sqm_mean', cmap = 'Blues', linewidth = 0.8, ax = ax, edgecolor = '0.3', legend = True)\n",
    "\n",
    "plt.title('Price per square meter by District')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and the target variable (Y).\n",
    "X = df.drop(columns = ['No', 'Price'])\n",
    "y = df['Price']\n",
    "\n",
    "# Standardize data \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split dataset into 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa lớp LinearRegressionCustom\n",
    "\n",
    "class LinearRegressionCustom:\n",
    "    # Khởi tạo lớp với tham số fit_intercept (mặc định là True)\n",
    "    def __init__(self, fit_intercept = True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    # Hàm fit để huấn luyện mô hình\n",
    "    def fit(self, X, y):\n",
    "        # Nếu fit_intercept là True, thêm cột 1 vào X để tính hệ số intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        # Tính toán hệ số hồi quy bằng công thức hồi quy tuyến tính\n",
    "        self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    # Hàm predict để dự đoán giá trị\n",
    "    def predict(self, X):\n",
    "        # Nếu fit_intercept là True, thêm cột 1 vào X để tính hệ số intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        # Tính toán giá trị dự đoán\n",
    "        return X @ self.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_custom = LinearRegressionCustom(fit_intercept = True)\n",
    "linear_custom.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_custom = linear_custom.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Custom Linear Regression - MSE:\", mean_squared_error(y_test, y_test_pred_custom))\n",
    "print(\"Custom Linear Regression - MAE:\", mean_absolute_error(y_test, y_test_pred_custom))\n",
    "print(\"Custom Linear Regression - R^2:\", r2_score(y_test, y_test_pred_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize = (7, 4))\n",
    "sns.scatterplot(x = y_test, y = y_test_pred_custom, color = 'lightgreen')\n",
    "\n",
    "# Plot a line for perfect predictions\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color = 'darkgreen', linestyle = '-')\n",
    "\n",
    "plt.title('Actual vs. Predicted House Prices (Linear Regression)')\n",
    "plt.xlabel('Actual House Prices')\n",
    "plt.ylabel('Predicted House Prices')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_y_pred = ridge.predict(X_test)\n",
    "\n",
    "print(\"Ridge Regression - MSE:\", mean_squared_error(y_test, ridge_y_pred))\n",
    "print(\"Ridge Regression - MAE:\", mean_absolute_error(y_test, ridge_y_pred))\n",
    "print(\"Ridge Regression - R^2:\", r2_score(y_test, ridge_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression với GridSearchCV\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv = 5)\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "ridge_y_pred_best = best_ridge.predict(X_test)\n",
    "\n",
    "print(\"Best Ridge Regression - MSE:\", mean_squared_error(y_test, ridge_y_pred_best))\n",
    "print(\"Best Ridge Regression - MAE:\", mean_absolute_error(y_test, ridge_y_pred_best))\n",
    "print(\"Best Ridge Regression - R^2:\", r2_score(y_test, ridge_y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot for Lasso Regression\n",
    "sns.scatterplot(x = y_test, y = ridge_y_pred, color = 'lightblue', ax = axes[0])\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color = 'blue', linestyle = '-')\n",
    "axes[0].set_title('Actual vs. Predicted House Prices (Ridge Regression)')\n",
    "axes[0].set_xlabel('Actual House Prices')\n",
    "axes[0].set_ylabel('Predicted House Prices')\n",
    "\n",
    "# Plot for Best Lasso Regression\n",
    "sns.scatterplot(x = y_test, y = ridge_y_pred_best, color = 'lightblue', ax = axes[1])\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color = 'blue', linestyle = '-')\n",
    "axes[1].set_title('Actual vs. Predicted House Prices (Best Ridge Regression)')\n",
    "axes[1].set_xlabel('Actual House Prices')\n",
    "axes[1].set_ylabel('Predicted House Prices')\n",
    "\n",
    "\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_y_pred = lasso.predict(X_test)\n",
    "\n",
    "print(\"Lasso Regression - MSE:\", mean_squared_error(y_test, lasso_y_pred))\n",
    "print(\"Lasso Regression - MAE:\", mean_absolute_error(y_test, lasso_y_pred))\n",
    "print(\"Lasso Regression - R^2:\", r2_score(y_test, lasso_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression với GridSearchCV\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "lasso_grid = GridSearchCV(Lasso(), lasso_params, cv = 5)\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "lasso_y_pred_best = best_lasso.predict(X_test)\n",
    "\n",
    "print(\"Best Lasso Regression - MSE:\", mean_squared_error(y_test, lasso_y_pred_best))\n",
    "print(\"Best Lasso Regression - MAE:\", mean_absolute_error(y_test, lasso_y_pred_best))\n",
    "print(\"Best Lasso Regression - R^2:\", r2_score(y_test, lasso_y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize = (14, 4))\n",
    "\n",
    "# Plot for Lasso Regression\n",
    "sns.scatterplot(x = y_test, y = lasso_y_pred, color = 'pink', ax = axes[0])\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color = 'deeppink', linestyle = '-')\n",
    "axes[0].set_title('Actual vs. Predicted House Prices (Lasso Regression)')\n",
    "axes[0].set_xlabel('Actual House Prices')\n",
    "axes[0].set_ylabel('Predicted House Prices')\n",
    "\n",
    "# Plot for Best Lasso Regression\n",
    "sns.scatterplot(x = y_test, y = lasso_y_pred_best, color = 'pink', ax = axes[1])\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color = 'deeppink', linestyle = '-')\n",
    "axes[1].set_title('Actual vs. Predicted House Prices (Best Lasso Regression)')\n",
    "axes[1].set_xlabel('Actual House Prices')\n",
    "axes[1].set_ylabel('Predicted House Prices')\n",
    "\n",
    "\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Compare 3 Regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo bảng so sánh\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Ridge Regression', 'Lasso Regression'],\n",
    "    'MAE': [mean_absolute_error(y_test, y_test_pred_custom), mean_absolute_error(y_test, ridge_y_pred_best), mean_absolute_error(y_test, lasso_y_pred_best)],\n",
    "    'MSE': [mean_squared_error(y_test, y_test_pred_custom), mean_squared_error(y_test, ridge_y_pred_best), mean_squared_error(y_test, lasso_y_pred_best)],\n",
    "    'R^2': [r2_score(y_test, y_test_pred_custom), r2_score(y_test, ridge_y_pred_best), r2_score(y_test, lasso_y_pred_best)]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Decision Tree and Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia tập thành các biến đặc trưng (X) và biến mục tiêu (Y)\n",
    "X = df.drop('Price', axis = 1)  # Đặc trưng\n",
    "Y = df['Price']  # Biến mục tiêu\n",
    "\n",
    "# Chia tập dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature = None, threshold = None, left = None, right = None, value = None):\n",
    "        self.feature = feature      # chỉ số của đặc trưng để chia\n",
    "        self.threshold = threshold  # ngưỡng để chia đặc trưng\n",
    "        self.left = left            # cây con bên trái (nhỏ hơn hoặc bằng ngưỡng)\n",
    "        self.right = right          # cây con bên phải (lớn hơn ngưỡng)\n",
    "        self.value = value          # giá trị của nút lá (trung bình giá trị y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth = None, random_state = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.tree_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs, self.tree_) for inputs in X])\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth = 0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or len(set(y)) == 1:\n",
    "            return Node(value=np.mean(y))\n",
    "\n",
    "        feature_indices = np.random.choice(n_features, n_features, replace = False)\n",
    "        best_feature, best_threshold = self._best_criteria(X, y, feature_indices)\n",
    "\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        if not np.any(left_indices) or not np.any(right_indices):\n",
    "            return Node(value = np.mean(y))\n",
    "\n",
    "        left_tree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return Node(best_feature, best_threshold, left_tree, right_tree)\n",
    "\n",
    "    def _best_criteria(self, X, y, feature_indices):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        for feature_idx in feature_indices:\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feature_idx], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature_idx\n",
    "                    split_threshold = threshold\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_values, threshold):\n",
    "        parent_loss = self._mse(y)\n",
    "        left_indices = feature_values <= threshold\n",
    "        right_indices = ~left_indices\n",
    "        if not np.any(left_indices) or not np.any(right_indices):\n",
    "            return 0\n",
    "        left_loss = self._mse(y[left_indices])\n",
    "        right_loss = self._mse(y[right_indices])\n",
    "        n = len(y)\n",
    "        n_left = len(y[left_indices])\n",
    "        n_right = len(y[right_indices])\n",
    "        child_loss = (n_left / n) * left_loss + (n_right / n) * right_loss\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _mse(self, y):\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "    def _predict(self, inputs, node):\n",
    "        while node.left:\n",
    "            if inputs[node.feature] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_estimators = 100, max_depth = None, max_features = 'sqrt', random_state = None, n_jobs = -1):\n",
    "        \"\"\"\n",
    "        Khởi tạo mô hình RandomForestRegressor.\n",
    "\n",
    "        Tham số:\n",
    "        - n_estimators (int): Số lượng cây quyết định trong rừng.\n",
    "        - max_depth (int): Độ sâu tối đa của mỗi cây quyết định. Nếu None, các cây sẽ được phát triển đầy đủ.\n",
    "        - max_features (str): Số lượng đặc trưng tối đa được xem xét để phân chia tại mỗi nút.\n",
    "        - random_state (int): Seed để đảm bảo tính nhất quán của random state.\n",
    "        - n_jobs (int): Số lượng công việc song song hóa.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.estimators = []\n",
    "    \n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Tạo một mẫu bootstrap từ dữ liệu gốc.\n",
    "\n",
    "        Tham số:\n",
    "        - X (numpy array, shape = [n_samples, n_features]): Dữ liệu đầu vào.\n",
    "        - y (numpy array, shape = [n_samples]): Nhãn của dữ liệu huấn luyện.\n",
    "\n",
    "        Returns:\n",
    "        - X_sample (numpy array, shape = [n_samples, n_features]): Mẫu bootstrap của dữ liệu đầu vào.\n",
    "        - y_sample (numpy array, shape = [n_samples]): Nhãn của mẫu bootstrap.\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace = True)\n",
    "        return X[indices], y[indices]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Huấn luyện mô hình RandomForest dựa trên dữ liệu huấn luyện.\n",
    "\n",
    "        Tham số:\n",
    "        - X (numpy array, shape = [n_samples, n_features]): Dữ liệu đầu vào.\n",
    "        - y (numpy array, shape = [n_samples]): Nhãn của dữ liệu huấn luyện.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        def _fit_single_tree(random_state):\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree = DecisionTreeRegressor(max_depth = self.max_depth, random_state = random_state)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            return tree\n",
    "        \n",
    "        random_states = np.random.randint(0, 10000, size = self.n_estimators)\n",
    "        self.estimators = Parallel(n_jobs = self.n_jobs)(delayed(_fit_single_tree)(rs) for rs in random_states)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Dự đoán giá trị cho dữ liệu đầu vào mới.\n",
    "\n",
    "        Tham số:\n",
    "        - X (numpy array, shape = [n_samples, n_features]): Dữ liệu đầu vào cần dự đoán.\n",
    "\n",
    "        Returns:\n",
    "        - predictions (numpy array, shape = [n_samples]): Giá trị dự đoán tương ứng với mỗi mẫu đầu vào.\n",
    "        \"\"\"\n",
    "        all_predictions = np.zeros((len(X), self.n_estimators))\n",
    "        for i, tree in enumerate(self.estimators):\n",
    "            all_predictions[:, i] = tree.predict(X)\n",
    "        return np.mean(all_predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Tìm kiếm trên lưới để tối ưu hóa độ sâu  trong mô hình hồi quy.\n",
    "\n",
    "    Tham số:\n",
    "    - X_train (numpy array): Dữ liệu huấn luyện (features).\n",
    "    - y_train (numpy array): Nhãn của dữ liệu huấn luyện.\n",
    "    - X_test (numpy array): Dữ liệu kiểm tra (features).\n",
    "    - y_test (numpy array): Nhãn của dữ liệu kiểm tra.\n",
    "    - max_depths (list): Danh sách các giá trị độ sâu để thử nghiệm.\n",
    "\n",
    "    Returns:\n",
    "    - best_depth (int): Độ sâu tối ưu.\n",
    "    - best_mse (float): Giá trị MSE tối ưu tương ứng với độ sâu tối ưu.\n",
    "\"\"\"\n",
    "\n",
    "def simple_grid_search_decision_tree(X_train, y_train, X_test, y_test, max_depths):\n",
    "    \n",
    "    best_depth = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for depth in max_depths:\n",
    "        model = DecisionTreeRegressor(max_depth = depth, random_state = 42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_depth = depth\n",
    "\n",
    "    return best_depth, best_mse\n",
    "\n",
    "\n",
    "def simple_grid_search_random_forest(X_train, y_train, X_test, y_test, max_depths):\n",
    "\n",
    "    best_depth = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for depth in max_depths:\n",
    "        model = RandomForestRegressor(max_depth = depth, random_state = 42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_depth = depth\n",
    "\n",
    "    return best_depth, best_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tìm giá trị max_depth tốt nhất\n",
    "max_depths = [None, 10, 20, 30, 40, 50]\n",
    "best_depth_dt, best_mse_dt = simple_grid_search_decision_tree(X_train.values, Y_train.values, X_test.values, Y_test.values, max_depths)\n",
    "print(f\"Best max_depth for DecisionTree: {best_depth_dt}, Best MSE: {best_mse_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth_rf, best_mse_rf = simple_grid_search_random_forest(X_train.values, Y_train.values, X_test.values, Y_test.values, max_depths)\n",
    "print(f\"Best max_depth for RandomForest: {best_depth_rf}, Best MSE: {best_mse_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng DecisionTree bạn đã định nghĩa để huấn luyện mô hình\n",
    "tree = DecisionTreeRegressor(max_depth = 20, random_state = 42)\n",
    "tree.fit(X_train.values, Y_train.values)\n",
    "\n",
    "# Dự đoán trên tập kiểm tra\n",
    "Y_pred_tree = tree.predict(X_test.values)\n",
    "\n",
    "# Đánh giá hiệu suất của mô hình\n",
    "\n",
    "print(\"Decision Tree Regression - MSE:\", mean_squared_error(Y_test.values, Y_pred_tree))\n",
    "print(\"Decision Tree Regression - MAE:\", mean_absolute_error(Y_test.values, Y_pred_tree))\n",
    "print(\"Decision Tree Regression - R^2:\", r2_score(Y_test.values, Y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện mô hình Random Forest\n",
    "random_forest = RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "random_forest.fit(X_train.values, Y_train.values)  # Huấn luyện trên tập huấn luyện\n",
    "\n",
    "# Dự đoán giá nhà trên tập kiểm tra\n",
    "Y_pred_forest = random_forest.predict(X_test.values)\n",
    "\n",
    "# Đánh giá hiệu suất của mô hình\n",
    "\n",
    "print(\"Random Forest Regression - MSE:\", mean_squared_error(Y_test.values, Y_pred_forest))\n",
    "print(\"Random Forest Regression - MAE:\", mean_absolute_error(Y_test.values, Y_pred_forest))\n",
    "print(\"Random Forest Regression - R^2:\", r2_score(Y_test.values, Y_pred_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.scatterplot(x = Y_test, y = Y_pred_tree, color = 'pink')\n",
    "\n",
    "# Plot a line for perfect predictions\n",
    "plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], color = 'red', linestyle = '-')\n",
    "\n",
    "plt.title('Actual vs. Predicted House Prices (Decision Tree)')\n",
    "plt.xlabel('Actual House Prices')\n",
    "plt.ylabel('Predicted House Prices')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.scatterplot(x = Y_test, y = Y_pred_forest, color = 'lightblue')\n",
    "\n",
    "# Plot a line for perfect predictions\n",
    "plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], color = 'blue', linestyle = '-')\n",
    "\n",
    "plt.title('Actual vs. Predicted House Prices (Random Forest)')\n",
    "plt.xlabel('Actual House Prices')\n",
    "plt.ylabel('Predicted House Prices')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DISCUSSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Tổng quan về Decision Tree**\n",
    "\n",
    "* Decision Tree (Cây quyết định) là một mô hình học máy phân loại và hồi quy. Mô hình này sử dụng cấu trúc cây, trong đó mỗi nút bên trong đại diện cho một thuộc tính (đặc trưng) và mỗi nhánh đại diện cho một giá trị của thuộc tính đó. Các nút lá thể hiện các giá trị mục tiêu (kết quả). Mô hình cây quyết định có thể được sử dụng cho cả bài toán phân loại và hồi quy.\n",
    "\n",
    "\n",
    "**2. Lí do chọn mô hình hồi quy**\n",
    "\n",
    "* Bản chất của biến mục tiêu: Giá nhà là giá trị liên tục, không phải nhãn phân loại.\n",
    "\n",
    "* Yêu cầu về đầu ra: Cần dự đoán giá trị cụ thể, không phải nhóm giá.\n",
    "\n",
    "* Phân giải chi tiết hơn: Hồi quy dự đoán giá trị cụ thể với độ chính xác cao hơn.\n",
    "\n",
    "* Phân loại không phù hợp với giá trị liên tục: Chia giá nhà thành nhóm làm mất thông tin chi tiết.\n",
    "\n",
    "* Tính chất của mô hình phân loại: Phân loại phù hợp với biến mục tiêu rời rạc, hữu hạn lớp.\n",
    "\n",
    "=> Hồi quy là lựa chọn tự nhiên và phù hợp cho dự đoán giá trị liên tục như giá nhà.\n",
    "\n",
    "**3. Cách thức hoạt động của Decision Tree** \n",
    "\n",
    "Cây quyết định hoạt động bằng cách chia dữ liệu thành các tập con nhỏ hơn dựa trên các điều kiện. Quá trình chia này tiếp tục lặp lại trên mỗi tập con mới cho đến khi đạt được các điều kiện dừng, chẳng hạn như khi tất cả các điểm dữ liệu trong một nút thuộc về cùng một lớp hoặc khi đạt đến độ sâu tối đa của cây. Các bước cơ bản của việc xây dựng cây quyết định bao gồm:\n",
    "\n",
    "* Chọn thuộc tính tốt nhất để chia dữ liệu tại mỗi bước. Thuộc tính tốt nhất được chọn dựa trên một tiêu chí, chẳng hạn như thông tin thu được hoặc giảm phương sai.\n",
    "\n",
    "* Chia dữ liệu thành các tập con dựa trên giá trị của thuộc tính được chọn.\n",
    "\n",
    "* Lặp lại quá trình trên các tập con cho đến khi đạt được điều kiện dừng.\n",
    "\n",
    "**4. Ưu điểm và nhược điểm của Decision Tree**\n",
    "- **Ưu điểm của Decision Tree:**\n",
    "   + Dễ hiểu và giải thích: Cây quyết định có cấu trúc trực quan dễ dàng để con người hiểu và giải thích.\n",
    "   + Xử lý tốt dữ liệu không tuyến tính: Cây quyết định không yêu cầu mối quan hệ tuyến tính giữa các thuộc tính và mục tiêu, giúp xử lý tốt dữ liệu phức tạp và không tuyến tính.\n",
    "   + Có thể xử lý cả dữ liệu phân loại và hồi quy: Cây quyết định có thể áp dụng cho cả hai loại bài toán.\n",
    "\n",
    "- **Nhược điểm của Decision Tree:**\n",
    "   + Dễ bị overfitting: Nếu không được cắt tỉa (prune) đúng cách, cây quyết định có thể quá phù hợp với dữ liệu huấn luyện, dẫn đến kém hiệu quả trên dữ liệu mới.\n",
    "   + Không ổn định: Một thay đổi nhỏ trong dữ liệu có thể dẫn đến một cây quyết định hoàn toàn khác biệt.\n",
    "   + Thiếu tính liên tục**: Các dự đoán từ cây quyết định thường có tính gián đoạn, không mượt mà như các phương pháp hồi quy.\n",
    "\n",
    "**5. So sánh với Linear Regression:**\n",
    "- **Tính đơn giản và dễ hiểu**: Linear Regression cũng dễ hiểu và giải thích nhưng yêu cầu mối quan hệ tuyến tính giữa các thuộc tính và mục tiêu.\n",
    "- **Khả năng mô hình hóa**: Linear Regression thích hợp hơn cho các dữ liệu có mối quan hệ tuyến tính, trong khi Decision Tree có thể xử lý tốt các mối quan hệ phi tuyến tính.\n",
    "- **Overfitting**: Linear Regression ít bị overfitting hơn so với Decision Tree nếu dữ liệu có nhiều nhiễu.\n",
    "- **Hiệu suất**: Linear Regression thường hoạt động tốt trên các dữ liệu có nhiều thuộc tính hơn so với Decision Tree, do cây quyết định có thể gặp vấn đề với dữ liệu có độ chiều cao."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Tổng quan về Random Forest**\n",
    "\n",
    "* Random Forest là một mô hình học máy mạnh mẽ và linh hoạt, sử dụng nhiều cây quyết định (Decision Trees) để đưa ra dự đoán. Mô hình này kết hợp dự đoán của nhiều cây quyết định để cải thiện độ chính xác và độ ổn định. Random Forest có thể được sử dụng cho cả bài toán phân loại và hồi quy.\n",
    "\n",
    "**2. Lí do chọn mô hình hồi quy**\n",
    "\n",
    "* Bản chất của biến mục tiêu: Giá nhà là giá trị liên tục, không phải nhãn phân loại.\n",
    "* Yêu cầu về đầu ra: Cần dự đoán giá trị cụ thể, không phải nhóm giá.\n",
    "* Phân giải chi tiết hơn: Hồi quy dự đoán giá trị cụ thể với độ chính xác cao hơn.\n",
    "* Phân loại không phù hợp với giá trị liên tục: Chia giá nhà thành nhóm làm mất thông tin chi tiết.\n",
    "* Tính chất của mô hình phân loại: Phân loại phù hợp với biến mục tiêu rời rạc, hữu hạn lớp.\n",
    "\n",
    "=> Hồi quy là lựa chọn tự nhiên và phù hợp cho dự đoán giá trị liên tục như giá nhà.\n",
    "\n",
    "**3. Cách thức hoạt động của Random Forest**\n",
    "\n",
    "Random Forest hoạt động bằng cách xây dựng nhiều cây quyết định trên các mẫu con khác nhau của dữ liệu. Mỗi cây quyết định trong rừng đưa ra dự đoán và Random Forest sẽ lấy trung bình các dự đoán này để đưa ra kết quả cuối cùng. Các bước cơ bản của việc xây dựng Random Forest bao gồm:\n",
    "\n",
    "* Chọn các mẫu con từ dữ liệu gốc bằng phương pháp lấy mẫu có hoàn lại (bootstrap sampling).\n",
    "* Xây dựng cây quyết định trên mỗi mẫu con. Tại mỗi nút, chỉ xem xét một tập hợp con ngẫu nhiên các thuộc tính để chia dữ liệu.\n",
    "* Kết hợp dự đoán của tất cả các cây quyết định bằng cách lấy trung bình (cho bài toán hồi quy) hoặc theo số phiếu bầu (cho bài toán phân loại).\n",
    "\n",
    "**4. Ưu điểm và nhược điểm của Random Forest**\n",
    "\n",
    "- **Ưu điểm của Random Forest:**\n",
    "   + Độ chính xác cao: Sử dụng nhiều cây quyết định giúp cải thiện độ chính xác của mô hình.\n",
    "   + Giảm thiểu overfitting: Việc kết hợp nhiều cây quyết định giúp giảm thiểu hiện tượng overfitting so với việc sử dụng một cây quyết định duy nhất.\n",
    "   + Ổn định: Random Forest ít nhạy cảm với các thay đổi nhỏ trong dữ liệu đầu vào.\n",
    "   + Xử lý tốt dữ liệu không tuyến tính: Random Forest không yêu cầu mối quan hệ tuyến tính giữa các thuộc tính và mục tiêu.\n",
    "\n",
    "- **Nhược điểm của Random Forest:**\n",
    "   + Độ phức tạp tính toán: Random Forest yêu cầu nhiều tài nguyên tính toán hơn so với một cây quyết định đơn lẻ.\n",
    "   + Khả năng diễn giải: Khó diễn giải hơn so với một cây quyết định đơn lẻ do sử dụng nhiều cây.\n",
    "\n",
    "**5. So sánh với Linear Regression**\n",
    "\n",
    "- **Tính đơn giản và dễ hiểu:** Linear Regression dễ hiểu và giải thích hơn, nhưng yêu cầu mối quan hệ tuyến tính giữa các thuộc tính và mục tiêu.\n",
    "- **Khả năng mô hình hóa:** Linear Regression thích hợp hơn cho các dữ liệu có mối quan hệ tuyến tính, trong khi Random Forest có thể xử lý tốt các mối quan hệ phi tuyến tính.\n",
    "- **Overfitting:** Linear Regression ít bị overfitting hơn so với một cây quyết định đơn lẻ, nhưng Random Forest giảm thiểu được overfitting nhờ kết hợp nhiều cây.\n",
    "- **Hiệu suất:** Linear Regression hoạt động tốt trên các dữ liệu có nhiều thuộc tính hơn so với một cây quyết định đơn lẻ, nhưng Random Forest có thể xử lý tốt dữ liệu có độ chiều cao.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONCLUSION AND RECOMMENDATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **REFERENCES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.zenrows.com/blog/web-crawler-python#initial-crawling-script\n",
    "2. https://viblo.asia/p/web-crawling-voi-thu-vien-beautifulsoup-1VgZvNGOZAw\n",
    "3. https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "4. https://www.topcoder.com/thrive/articles/web-scraping-with-beautiful-soup\n",
    "5. https://www.w3schools.com/python/python_regex.asp\n",
    "6. https://docs.python.org/3/howto/regex.html\n",
    "7. https://peps.python.org/pep-0008/#function-and-variable-names\n",
    "8. https://cs230.stanford.edu/syllabus/\n",
    "9. https://www.geeksforgeeks.org/cross-validation-machine-learning/\n",
    "10. https://miai.vn/2021/01/18/k-fold-cross-validation-tuyet-chieu-train-khi-it-du-lieu/\n",
    "11. https://machinelearningcoban.com/tabml_book/ch_data_processing/eda.html#\n",
    "12. https://www.ibm.com/topics/exploratory-data-analysis\n",
    "13. https://scikit-learn.org/stable/modules/density.html\n",
    "14. https://viblo.asia/p/danh-gia-cac-mo-hinh-hoc-may-RnB5pp4D5PG\n",
    "15. https://www.geeksforgeeks.org/k-nearest-neighbours/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ABOUT US**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Members**\n",
    "| **ID**| **Name**| **Major**| **University**|\n",
    "|-|-|-|-|\n",
    "| 22280094 | Le Thanh Thuy | Data Science  | University of Science (VNUHCM) |\n",
    "| 22280091 | Nguyen Ngoc Thanh Thu | Data Science  | University of Science (VNUHCM) |\n",
    "| 22280070 | Phan Binh Phuong | Data Science  | University of Science (VNUHCM) |\n",
    "| 22280056 | Luong Thanh Nam| Data Science  | University of Science (VNUHCM) |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
